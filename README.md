На основе предоставленных лекционных материалов (слайды) и учебных пособий (методички Лаврова, Плаксия, Северина), ниже представлены развернутые ответы на вопросы билетов. Ответы сформулированы в академическом стиле, структурированы и являются самодостаточными для каждого билета.

---

### Билет №1

**1. Методы второго порядка в задачах оптимизации с одной переменной. (Метод Ньютона)**

*   **Теоретическая часть:**
    Метод Ньютона относится к итерационным методам второго порядка, так как использует информацию не только о первой производной (скорости изменения функции), но і о второй производной (кривизне функции). Основная идея метода заключается в квадратичной аппроксимации целевой функции $f(x)$ в окрестности текущей точки $x_k$ с помощью ряда Тейлора, удержанием членов до второго порядка включительно. Следующая точка приближения ищется как точка минимума этой аппроксимирующей параболы. Метод обладает квадратичной скоростью сходимости, что обеспечивает быстрое нахождение экстремума, если начальное приближение выбрано достаточно близко к оптимуму и функция является выпуклой.

*   **Формула:**
    Разложение функции в ряд Тейлора в окрестности $x_k$:
    $f(x) \approx f(x_k) + f'(x_k)(x - x_k) + \frac{1}{2}f''(x_k)(x - x_k)^2$.
    Необходимое условие экстремума для аппроксимирующей функции — равенство нулю её первой производной по $x$:
    $f'(x_k) + f''(x_k)(x - x_k) = 0$.
    Отсюда выводится итерационная формула метода Ньютона:
    $$x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}$$
    где $x_k$ — текущее приближение, $f'(x_k)$ — первая производная, $f''(x_k)$ — вторая производная. Условие применимости: $f''(x_k) \neq 0$.

*   **Примеры:**
    Данный метод применяется в инженерных расчетах для уточнения корней уравнений (где $f(x)$ — это производная оптимизируемой функции) и быстрой оптимизации гладких функций, например, при расчете параметров балок на изгиб при условии, что аналитическое выражение функции энергии деформации известно и дважды дифференцируемо.

**2. Общая характеристика методов n-мерной оптимизации нулевого порядка**

*   **Теоретическая часть:**
    Методы нулевого порядка (или методы прямого поиска) предназначены для минимизации функции многих переменных $f(\vec{x})$, где $\vec{x} \in R^n$, используя только значения самой целевой функции, без вычисления её градиента (производных). Эти методы применяются, когда целевая функция недифференцируема, имеет разрывы, задана алгоритмически (в виде "черного ящика") или зашумлена. К основным стратегиям относятся детерминированный поиск по образцу и методы деформируемого многогранника.

*   **Формула:**
    Общая схема итерации для таких методов может быть записана как:
    $$\vec{x}_{k+1} = \vec{x}_k + \lambda_k \vec{p}_k$$
    где $\vec{p}_k$ — направление поиска, выбранное на основе сравнения значений функции $f(\vec{x})$ в пробных точках (например, в вершинах симплекса или по координатным осям), а $\lambda_k$ — шаг поиска. Градиент $\nabla f$ здесь не фигурирует.

*   **Примеры:**
    Применяются при оптимизации сложных инженерных конструкций, где зависимость характеристик (прочность, вес) от параметров (толщины, углы) вычисляется с помощью тяжелых численных моделей (МКЭ), и аналитическое вычисление производных невозможно. Примером конкретного алгоритма является метод Нелдера-Мида или метод Хука-Дживса.

---

### Билет №2

**1. Метод парабол в задачах оптимизации с одной переменной**

*   **Теоретическая часть:**
    Метод парабол (квадратичной интерполяции) относится к методам полиномиальной аппроксимации. Он базируется на предположении, что в окрестности точки минимума унимодальная функция $f(x)$ может быть с достаточной точностью аппроксимирована полиномом второй степени (параболой). Алгоритм (метод Пауэлла) требует наличия трех пробных точек $x_1, x_2, x_3$. По значениям функции в этих точках строится интерполяционный полином Лагранжа, и аналитически находится его минимум, который принимается за новое приближение оптимума целевой функции.

*   **Формула:**
    Координата минимума аппроксимирующей параболы $\tilde{x}$ вычисляется через значения в трех точках ($x_1 < x_2 < x_3$):
    $$\tilde{x} = \frac{1}{2} \frac{(x_2^2 - x_3^2)f(x_1) + (x_3^2 - x_1^2)f(x_2) + (x_1^2 - x_2^2)f(x_3)}{(x_2 - x_3)f(x_1) + (x_3 - x_1)f(x_2) + (x_1 - x_2)f(x_3)}$$
    Если знаменатель равен нулю, точки лежат на одной прямой.

*   **Примеры:**
    Используется в подпрограммах одномерного поиска для многомерных методов оптимизации (например, в методе сопряженных градиентов), когда необходимо быстро найти минимум вдоль направления спуска, а вычисление производной трудоемко.

**2. Необходимые и достаточные условия в многомерных задачах оптимизации**

*   **Теоретическая часть:**
    Для дважды дифференцируемой функции многих переменных $f(\vec{x})$ условия локального безусловного экстремума формулируются через градиент и матрицу Гессе.
    *   **Необходимое условие первого порядка:** В точке локального экстремума $\vec{x}^*$ градиент функции равен нулю (точка стационарна).
    *   **Необходимое условие второго порядка (для минимума):** В точке $\vec{x}^*$ квадратичная форма второго дифференциала неотрицательна, то есть матрица Гессе $H(\vec{x}^*)$ положительно полуопределена.
    *   **Достаточное условие (для минимума):** Если в стационарной точке матрица Гессе положительно определена (строго), то это точка строгого локального минимума.

*   **Формула:**
    Необходимое условие (стационарность):
     ```math
    \nabla f(\vec{x}^*) = \left\{ \frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_n} \right\}^T = \vec{0}
    ```
    Достаточное условие (через критерий Сильвестра для матрицы Гессе $H$):
    $$H = \left[ \frac{\partial^2 f}{\partial x_i \partial x_j} \right], \quad \Delta_k > 0 \quad \forall k=1\dots n$$
    где $\Delta_k$ — главные угловые миноры матрицы Гессе.

*   **Примеры:**
    При проектировании поверхности сложной формы (например, крыла самолета), эти условия позволяют проверить, является ли найденная численным методом точка действительно точкой минимального аэродинамического сопротивления, а не седловой точкой.

---

### Билет №3

**1. Методы первого порядка в задачах оптимизации с одной переменной (Метод дихотомии, метод секущих)**

*   **Теоретическая часть:**
    Методы первого порядка используют информацию о первой производной функции $f'(x)$.
    *   *Метод секущих (хорд):* Является модификацией метода Ньютона. Вместо вычисления второй производной $f''(x)$ используется её конечно-разностная аппроксимация по двум точкам. Метод ищет корень уравнения $f'(x)=0$.
    *   *Метод дихотомии (в контексте использования производной):* Если доступна производная, метод дихотомии применяется для поиска корня уравнения $f'(x)=0$. Отрезок делится пополам, вычисляется производная в центре. В зависимости от знака производной выбирается левая или правая половина (для выпуклой функции: если $f'(c) > 0$, минимум слева).

*   **Формула:**
    Метод секущих (для поиска нуля производной):
    $$x_{k+1} = x_k - \frac{f'(x_k)(x_k - x_{k-1})}{f'(x_k) - f'(x_{k-1})}$$
    Метод дихотомии (по производной):
    Если $f'(\frac{a+b}{2}) > 0$, то $b_{new} = \frac{a+b}{2}$, иначе $a_{new} = \frac{a+b}{2}$.

*   **Примеры:**
    Применяются при расчете оптимальных режимов работы оборудования, где зависимость эффективности от параметра (например, температуры) имеет гладкий характер, и можно вычислить скорость изменения эффективности (производную).

**2. Метод Бокса и Метод симплекса**

*   **Теоретическая часть:**
    Оба метода относятся к методам многомерной оптимизации нулевого порядка (прямого поиска).
    *   *Метод Симплекса (регулярного):* В пространстве параметров строится правильный многогранник (симплекс) из $n+1$ вершин. Вычисляются значения функции в вершинах. "Наихудшая" вершина зеркально отражается относительно центра тяжести остальных вершин, образуя новый симплекс.
    *   *Метод Бокса (Комплекс-метод):* Является развитием симплекс-метода для задач с ограничениями. Используется "комплекс" из $N > n+1$ вершин. Вершины, нарушающие ограничения или имеющие плохие значения функции, смещаются к центру тяжести комплекса.

*   **Формула:**
    Отражение в симплекс-методе:
    $$\vec{x}_{new} = \vec{x}_c + \alpha (\vec{x}_c - \vec{x}_{worst})$$
    где $\vec{x}_c$ — центр тяжести всех вершин, кроме худшей, $\vec{x}_{worst}$ — худшая вершина, $\alpha$ — коэффициент отражения (обычно 1).

*   **Примеры:**
    Используются для оптимизации состава химических смесей или параметров технологических процессов, где границы допустимых значений переменных (ограничения) играют ключевую роль, а функция зашумлена.

---

### Билет №4

**1. Метод золотого сечения. Сравнение методов нулевого порядка**

*   **Теоретическая часть:**
    Метод золотого сечения — это метод одномерной оптимизации нулевого порядка (без производных), основанный на принципе исключения интервалов. Отрезок неопределенности делится точками в пропорции "золотого сечения" ($\Phi \approx 1.618$). На каждой итерации длина интервала сокращается в $\approx 0.618$ раз.
    *Сравнение:* По сравнению с методом дихотомии (равномерного деления), метод золотого сечения более эффективен по количеству вычислений функции, так как на каждой новой итерации требуется вычислить значение функции только в одной новой точке (вторая точка наследуется с предыдущего шага), тогда как дихотомия требует двух вычислений на итерацию. Метод Фибоначчи еще эффективнее для фиксированного числа шагов, но метод золотого сечения проще в реализации.

*   **Формула:**
    Коэффициент золотого сечения: $\tau = \frac{\sqrt{5}-1}{2} \approx 0.618$.
    Точки разбиения интервала $[a, b]$:
    $x_1 = a + (1-\tau)(b-a)$, $x_2 = a + \tau(b-a)$.
    Условие сокращения интервала: $L_{k+1} = \tau L_k$.

*   **Примеры:**
    Применяется в численных методах как вспомогательная процедура линейного поиска (line search) для определения оптимального шага $\lambda$ в градиентных методах многомерной оптимизации.

**2. Метод Бокса и Метод симплекса**

*   **Теоретическая часть:**
    (Аналогично ответу в Билете №3).
    *   *Метод регулярного симплекса:* Использует жесткую структуру (правильный $n$-мерный многогранник). Движение происходит путем отражения худшей точки. Если отражение не дает улучшения, симплекс уменьшается (редукция).
    *   *Метод Бокса:* Адаптирован для условной оптимизации. Случайным образом генерируется $N$ точек (комплекс). Худшая точка смещается к центроиду остальных. Если точка нарушает ограничения, она сдвигается внутрь допустимой области.

*   **Формула:**
    Смещение в методе Бокса:
    $$\vec{x}_{new} = \frac{1}{N-1} \sum_{i \neq worst} \vec{x}_i + \alpha \left(\frac{1}{N-1} \sum_{i \neq worst} \vec{x}_i - \vec{x}_{worst}\right)$$
    Если $\vec{x}_{new}$ нарушает ограничения $x_{min} \le x \le x_{max}$, координата принудительно возвращается на границу или сдвигается внутрь на малую величину $\varepsilon$.

*   **Примеры:**
    Оптимизация рецептуры сплавов (метод Бокса позволяет учесть ограничения на процентное содержание компонентов), настройка контроллеров в робототехнике.

---

### Билет №5

**1. Метод дихотомии. Сравнение с равномерным делением**

*   **Теоретическая часть:**
    Метод дихотомии (половинного деления) — метод одномерной безусловной оптимизации нулевого порядка. На каждой итерации интервал неопределенности $[a, b]$ делится пополам. Для определения, в какой половине находится минимум, берутся две пробные точки, незначительно отстоящие от центра: $x_{1,2} = \frac{a+b}{2} \pm \delta$. Интервал сокращается примерно в 2 раза за итерацию.
    *Сравнение:* Метод равномерного деления (перебора) разбивает интервал сразу на $N$ частей и выбирает лучший, что гарантирует нахождение глобального минимума, но требует огромного количества вычислений. Дихотомия экспоненциально быстрее сходится к локальному минимуму, но требует унимодальности функции.

*   **Формула:**
    Центр интервала: $x_c = \frac{a+b}{2}$.
    Пробные точки: $x_1 = x_c - \delta$, $x_2 = x_c + \delta$.
    Сокращение интервала: $L_{k+1} \approx \frac{L_k}{2}$.
    Условие сходимости: $(b-a) < \varepsilon$.

*   **Примеры:**
    Используется для быстрого нахождения корня уравнения $f(x)=0$ или экстремума, когда вычисление функции дешевое, а требования к точности высокие, например, при расчете статического равновесия балки.

**2. Метод координатного спуска, овражные ситуации**

*   **Теоретическая часть:**
    Метод координатного спуска (Гаусса-Зейделя) — метод многомерной оптимизации нулевого порядка. Идея заключается в поочередной минимизации функции только по одной координате ($x_1$, затем $x_2$ и т.д.), фиксируя остальные.
    *Овражная ситуация:* Это случай, когда линии уровня функции сильно вытянуты (имеют вид оврага). Градиент направлен почти перпендикулярно дну оврага. В таких ситуациях метод координатного спуска начинает "рыскать" (зигзагообразное движение) и сходится крайне медленно, делая множество мелких шагов, не продвигаясь к минимуму вдоль дна оврага.

*   **Формула:**
    Алгоритм итерации:
    $$\vec{x}_{k+1} = \vec{x}_k + \lambda_k \vec{e}_i$$
    где $\vec{e}_i$ — единичный орт $i$-й оси координат. Шаг $\lambda_k$ находится путем решения одномерной задачи: $\min_{\lambda} f(\vec{x}_k + \lambda \vec{e}_i)$.

*   **Примеры:**
    Оптимизация параметров теплообменника (длина, диаметр труб), где переменные физически разнородны и удобно менять их по очереди. Плохо работает для функций типа Розенброка (классическая овражная функция).

---

### Билет №6

**1. Общая характеристика одномерных методов оптимизации нулевого порядка**

*   **Теоретическая часть:**
    Методы нулевого порядка используют только значения целевой функции $f(x)$ и не требуют вычисления производных. Они основаны на стратегии сужения интервала неопределенности. Ключевое требование к функции — унимодальность (наличие только одного экстремума на отрезке).
    Основные методы: равномерный поиск (перебор), поразрядный поиск, дихотомия, золотое сечение, метод Фибоначчі.
    Критерии сравнения: эффективность (коэффициент сокращения интервала за одно вычисление функции), надежность, простота реализации. Наиболее эффективным для фиксированного числа шагов является метод Фибоначчи, а наиболее универсальным — метод золотого сечения.

*   **Формула:**
    Общая задача: Найти $x^* = \arg \min_{x \in [a,b]} f(x)$.
    Условие унимодальности: для $x_1 < x_2$, если $f(x_1) < f(x_2)$, то минимум слева от $x_2$.

*   **Примеры:**
    Подбор оптимальной температуры в химическом реакторе, где зависимость выхода продукта от температуры получается экспериментально (значение функции), а формулы для дифференцирования нет.

**2. Овражные ситуации. Леммы и Теорема о сопряженных направлениях**

*   **Теоретическая часть:**
    Овражные ситуации возникают, когда собственные числа гессиана функции сильно различаются ($\lambda_{max} / \lambda_{min} \gg 1$), что приводит к вытянутым линиям уровня. Обычный градиентный спуск здесь неэффективен.
    Решение — использование *сопряженных направлений*. Два вектора $\vec{p}_i$ и $\vec{p}_j$ называются сопряженными относительно матрицы $H$ (Гессиана), если $\vec{p}_i^T H \vec{p}_j = 0$.
    *Теорема:* Если квадратичная функция минимизируется последовательным точным поиском вдоль системы $n$ линейно независимых, попарно сопряженных направлений, то минимум будет найден не более чем за $n$ шагов.

*   **Формула:**
    Условие сопряженности:
    $$(\vec{p}_i, H \vec{p}_j) = 0, \quad i \neq j$$
    Квадратичная функция: $f(\vec{x}) = \frac{1}{2}\vec{x}^T H \vec{x} + \vec{b}^T \vec{x} + c$.

*   **Примеры:**
    Методы сопряженных градиентов (Флетчера-Ривса) используются при обучении нейронных сетей и решении больших систем линейных уравнений, где пространство параметров имеет высокую размерность и "овражную" структуру поверхности ошибки.

---

### Билет №7

**1. Методы второго порядка в задачах оптимизации с одной переменной (Метод Ньютона)**

*   **Теоретическая часть:**
    (См. ответ в Билете №1). Метод Ньютона использует вторую производную для построения квадратичной модели функции. Это обеспечивает быструю сходимость вблизи экстремума. Однако метод требует вычисления $f''(x)$ и может расходиться, если начальная точка далека от оптимума или $f''(x) < 0$ (функция не выпукла).

*   **Формула:**
    Итерационный шаг:
    $$x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}$$
    Критерий остановки: $|f'(x_k)| < \varepsilon$ или $|x_{k+1} - x_k| < \varepsilon$.

*   **Примеры:**
    Расчет корней трансцендентных уравнений в теплофизике, где функции гладкие и известны аналитически.

**2. Градиентный метод и метод наискорейшего спуска**

*   **Теоретическая часть:**
    Градиентные методы — это методы первого порядка для многомерной оптимизации. Они используют вектор градиента $\nabla f(\vec{x})$, который указывает направление наибыстрейшего возрастания функции. Для минимизации движение осуществляется в сторону антиградиента ($-\nabla f$).
    *Метод наискорейшего спуска:* На каждой итерации выбирается направление антиградиента $\vec{p}_k = -\nabla f(\vec{x}_k)$. Шаг $\lambda_k$ выбирается оптимальным образом, то есть решается задача одномерной минимизации вдоль этого направления до нахождения минимума. В точке минимума новый градиент будет ортогонален предыдущему направлению движения.

*   **Формула:**
    Алгоритм:
    1. $\vec{p}_k = -\nabla f(\vec{x}_k)$.
    2. Найти $\lambda_k^* = \arg \min_{\lambda > 0} f(\vec{x}_k + \lambda \vec{p}_k)$.
    3. $\vec{x}_{k+1} = \vec{x}_k + \lambda_k^* \vec{p}_k$.

*   **Примеры:**
    Базовый метод обучения простых моделей машинного обучения (линейная регрессия), оптимизация формы простых механических деталей.

---

### Билет №8

**1. Методы первого порядка в задачах оптимизации с одной переменной (Метод дихотомии, метод секущих)**

*   **Теоретическая часть:**
    (Аналогично Билету №3, с уточнением классификации).
    Методы, использующие производные.
    *Метод средней точки (аналог дихотомии с производной):* Вычисляется производная в середине интервала. Если $f'(x) > 0$, корень (минимум) слева, исключаем правую часть.
    *Метод секущих:* Аппроксимирует кривую производной $f'(x)$ хордой, проходящей через две точки. Следующее приближение — точка пересечения хорды с осью абсцисс. Не требует вычисления второй производной (в отличие от Ньютона), но сходится медленнее (сверхлинейно, порядок $\approx 1.618$).

*   **Формула:**
    Метод секущих:
    $$x_{k+1} = x_k - f'(x_k) \frac{x_k - x_{k-1}}{f'(x_k) - f'(x_{k-1})}$$

*   **Примеры:**
    Используется в численных решателях (Solver) для поиска корней уравнений баланса в гидравлических системах.

**2. Метод сопряженных градиентов (Метод Флетчера-Ривса)**

*   **Теоретическая часть:**
    Метод Флетчера-Ривса — это метод первого порядка, который, однако, использует историю градиентов для построения сопряженных направлений. Это позволяет избежать "рыскания" метода наискорейшего спуска в оврагах. На первом шаге делается шаг по антиградиенту. На последующих шагах направление спуска является линейной комбинацией текущего антиградиента и предыдущего направления движения.

*   **Формула:**
    Направление поиска:
    $$\vec{p}_{k+1} = -\nabla f(\vec{x}_{k+1}) + \beta_k \vec{p}_k$$
    Коэффициент $\beta_k$ (Флетчера-Ривса):
    $$\beta_k = \frac{\|\nabla f(\vec{x}_{k+1})\|^2}{\|\nabla f(\vec{x}_k)\|^2}$$
    Шаг $\lambda$ определяется точным линейным поиском.

*   **Примеры:**
    Крупномасштабные задачи оптимизации (тысячи переменных), например, реконструкция изображений или томография, так как метод требует мало памяти (хранит только несколько векторов), в отличие от методов Ньютона, требующих хранения матрицы.

---

### Билет №9

**1. Метод золотого сечения. Сравнение методов нулевого порядка**

*   **Теоретическая часть:**
    (См. ответ в Билете №4).
    Метод золотого сечения оптимален среди методов, не требующих заранее известного числа итераций. Он гарантирует сокращение интервала неопределенности с коэффициентом $1/\Phi \approx 0.618$ на каждом шаге.
    *Сравнение:* Метод Фибоначчи теоретически более эффективен, но требует знать число шагов $N$ заранее и использовать числа Фибоначчи. Метод дихотомии требует двух вычислений функции на шаг. Золотое сечение — "золотая середина" по эффективности и простоте.

*   **Формула:**
    Точки: $x_1 = b - (b-a)/\Phi$, $x_2 = a + (b-a)/\Phi$.
    На каждом шаге вычисляется только одна новая точка, вторая берется из предыдущей итерации.

*   **Примеры:**
    Оптимизация параметров настройки PID-регулятора (один параметр), где качество переходного процесса вычисляется моделированием (дорогое вычисление).

**2. Методы n-мерной оптимизации второго порядка. Метод Ньютона. Модифицированный и Комбинированный методы**

*   **Теоретическая часть:**
    Методы второго порядка используют матрицу Гессе $H(\vec{x})$ (вторых производных).
    *   *Классический Метод Ньютона:* $ \vec{x}_{k+1} = \vec{x}_k - H^{-1} \nabla f$. Сходится квадратично, но требует вычисления и обращения Гессиана на каждом шаге.
    *   *Модифицированный метод Ньютона:* Гессиан вычисляется один раз в начальной точке и не обновляется (или обновляется редко). Сходимость линейная, но вычисления дешевле.
    *   *Метод Ньютона с регулировкой шага:* Направление берется ньютоновское, но длина шага $\lambda$ оптимизируется.
    *   *Метод Марквардта:* Комбинация градиентного спуска и метода Ньютона. К диагонали Гессиана добавляется параметр $\mu I$, делая матрицу положительно определенной и регулируя шаг.

*   **Формула:**
    Общая формула:
    $$\vec{x}_{k+1} = \vec{x}_k - \lambda_k [H(\vec{x}_k)]^{-1} \nabla f(\vec{x}_k)$$
    В классическом методе $\lambda_k = 1$.

*   **Примеры:**
    Точный расчет кинетики химических реакций или равновесия механических систем, где функция гладкая, выпуклая, а количество переменных невелико (до 100), что позволяет хранить Гессиан.

---

### Билет №10

**1. Общая характеристика одномерных методов оптимизации нулевого порядка**

*   **Теоретическая часть:**
    (См. ответ в Билете №6).
    Это итерационные методы сужения интервала неопределенности $[a, b]$ для унимодальной функции без вычисления производных. Суть: выбор пробных точек внутри интервала, сравнение значений $f(x)$ и отбрасывание части интервала, где минимума гарантированно нет.
    Классификация: пассивные (все точки выбираются сразу — равномерный поиск) и последовательные (активные, точки выбираются адаптивно — дихотомия, золотое сечение). Активные методы всегда эффективнее пассивных.

*   **Формула:**
    Коэффициент эффективности (уменьшения интервала) $E = \frac{L_0}{L_N}$.
    Для пассивного поиска $E \approx N/2$, для Фибоначчи $E \approx 1.618^N$.

*   **Примеры:**
    Настройка громкости или яркости прибора пользователем (человек сравнивает два состояния "лучше/хуже" и сужает диапазон поиска).

**2. Метод деформируемого многогранника (Метод Нелдера Мида)**

*   **Теоретическая часть:**
    Метод Нелдера-Мида (симплекс-метод) — эвристический метод нулевого порядка для многомерной оптимизации. Использует симплекс из $n+1$ вершин. На каждом шаге худшая вершина (с максимальным значением функции для задачи минимизации) заменяется на новую, полученную отражением через центр тяжести остальных вершин.
    Операции деформации симплекса позволяют ему адаптироваться к рельефу функции:
    1.  *Отражение* (базовый шаг).
    2.  *Растяжение* (если направление удачное, шаг увеличивается).
    3.  *Сжатие* (если отражение не дало улучшения).
    4.  *Редукция* (сжатие всего симплекса к лучшей точке).

*   **Формула:**
    Центр тяжести: $\vec{x}_c = \frac{1}{n} \sum_{i \neq h} \vec{x}_i$.
    Отражение: $\vec{x}_r = \vec{x}_c + \alpha(\vec{x}_c - \vec{x}_h)$.

*   **Примеры:**
    Широко используется в прикладных пакетах (MATLAB `fminsearch`, Excel Solver) для оптимизации сложных негладких функций, например, в экономике или биологии, где производные не определены.

---

### Билет №11

**1. Необходимые и достаточные условия в одномерных задачах оптимизации**

*   **Теоретическая часть:**
    Для дифференцируемой функции $f(x)$ одной переменной условия локального экстремума (минимума) в точке $x^*$ формулируются так:
    *   **Необходимое условие первого порядка:** Производная в точке экстремума равна нулю (точка стационарна): $f'(x^*) = 0$.
    *   **Достаточное условие второго порядка:** Если в стационарной точке $f'(x^*) = 0$ вторая производная $f''(x^*) > 0$, то это точка строгого локального минимума. Если $f''(x^*) < 0$ — максимума.
    *   *Замечание:* Если $f''(x^*) = 0$, требуются производные более высоких порядков.

*   **Формула:**
    Минимум: $f'(x^*) = 0$ и $f''(x^*) > 0$.
    Пример функции: $f(x) = x^2$. $f'(0) = 2\cdot0 = 0$, $f''(0) = 2 > 0$ $\rightarrow$ минимум.

*   **Примеры:**
    Определение оптимальной партии заказа в логистике (модель Вильсона), где функция затрат выпукла и легко дифференцируема.

**2. Линейное программирование. Каноническая форма записи**

*   **Теоретическая часть:**
    Линейное программирование (ЛП) занимается оптимизацией линейной целевой функции при линейных ограничениях.
    *Каноническая форма* — это стандартный вид задачи ЛП, необходимый для применения Симплекс-метода.
    Характеристики канонической формы:
    1.  Целевая функция максимизируется (или минимизируется).
    2.  Все ограничения имеют вид строгих равенств ($=$).
    3.  Все переменные неотрицательны ($x_j \ge 0$).
    Переход к канонической форме осуществляется путем введения дополнительных (слабых) переменных для превращения неравенств в равенства.

*   **Формула:**
    Целевая функция: $F(\vec{x}) = \sum c_j x_j \to \max$.
    Ограничения: $\sum a_{ij} x_j = b_i, \quad i=1\dots m$.
    Переменные: $x_j \ge 0, \quad j=1\dots n$.

*   **Примеры:**
    Задача планирования производства: максимизация прибыли при ограниченных ресурсах. Исходные ограничения типа "ресурсов использовано не больше, чем есть" ($Ax \le b$) переводятся в равенства ($Ax + s = b$) для решения симплекс-методом.

---

### Билет №12

**1. Основные элементы постановки задачи оптимизации**

*   **Теоретическая часть:**
    Математическая постановка любой задачи оптимизации включает три ключевых элемента:
    1.  **Варьируемые параметры (переменные управления) $\vec{u}$:** Величины, которые мы можем изменять для поиска лучшего решения.
    2.  **Целевая функция (Критерий качества) $J(\vec{u})$:** Скалярная функция, зависящая от параметров, значение которой нужно минимизировать или максимизировать (стоимость, вес, прочность).
    3.  **Ограничения (Область допустимых решений):** Условия, накладываемые на параметры. Бывают функциональными ($G_j(\vec{u}) \le 0$) и прямыми (граничными) $u_{min} \le u_i \le u_{max}$.
    Также важным элементом является *Математическая модель*, связывающая параметры с характеристиками объекта.

*   **Формула:**
    Найти $\vec{u}^* = \arg \min J(\vec{u})$
    при условиях: $G_j(\vec{u}) \le [G_j], \quad \vec{u} \in U$.

*   **Примеры:**
    Проектирование ферменной конструкции:
    $\vec{u}$ — площади сечения стержней.
    $J$ — вес конструкции (минимум).
    Ограничения — механические напряжения не превышают предел текучести.

**2. Линейное программирование. Метод Данцига**

*   **Теоретическая часть:**
    Метод Данцига, более известный как **Симплекс-метод**, является универсальным методом решения задач линейного программирования (ЛП).
    Геометрическая идея: Область допустимых решений задачи ЛП представляет собой выпуклый многогранник (политоп). Оптимум линейной функции всегда достигается в одной из вершин этого многогранника.
    Алгоритм метода Данцига:
    1.  Найти начальную вершину (опорный план).
    2.  Проверить, является ли она оптимальной.
    3.  Если нет, перейти к смежной вершине, в которой значение целевой функции лучше. Переход осуществляется вдоль ребра многогранника.
    4.  Повторять до нахождения оптимума. Метод конечен, так как число вершин конечно.

*   **Формула:**
    Используются симплекс-таблицы для выполнения преобразований Жордана-Гаусса над системой линейных уравнений ограничений. Критерий оптимальности: отсутствие положительных (для максимизации) коэффициентов в строке целевой функции для небазисных переменных.

*   **Примеры:**
    Транспортная задача (оптимизация перевозок), задача о диете (составление рациона минимальной стоимости), оптимизация портфеля ценных бумаг.

---

### Билет №13

**1. Метод парабол в задачах оптимизации с одной переменной**

*   **Теоретическая часть:**
    (См. ответ в Билете №2).
    Метод основан на замене сложной функции $f(x)$ параболой $P(x) = a x^2 + bx + c$, проходящей через три точки. Минимум параболы находится аналитически и принимается за следующее приближение минимума функции. Этот процесс повторяется итеративно. Метод сходится быстрее, чем золотое сечение, для гладких функций (сверхлинейная сходимость).

*   **Формула:**
    Аппроксимирующий минимум $\bar{x}$:
    $$\bar{x} = x_2 - \frac{1}{2} \frac{(x_2 - x_1)^2 [f(x_2) - f(x_3)] - (x_2 - x_3)^2 [f(x_2) - f(x_1)]}{(x_2 - x_1) [f(x_2) - f(x_3)] - (x_2 - x_3) [f(x_2) - f(x_1)]}$$
    (Это альтернативная форма записи формулы из Билета №2).

*   **Примеры:**
    Используется в инженерных расчетах, где вычисление функции очень трудоемко (например, требует проведения эксперимента или сложного CFD-расчета), и нужно найти оптимум за минимальное число шагов.

**2. Необходимые и достаточные условия в многомерных задачах оптимизации**

*   **Теоретическая часть:**
    (См. ответ в Билете №2).
    Для функции $f(\vec{x})$, $\vec{x} \in R^n$:
    1.  **Необходимое условие:** $\nabla f(\vec{x}^*) = 0$. Точка, где градиент равен нулю, называется стационарной.
    2.  **Достаточное условие:** Исследование матрицы вторых производных (Гессиана) $H$ в стационарной точке.
        *   Если $H$ положительно определена (все собственные числа > 0, или все главные миноры > 0) — это локальный **минимум**.
        *   Если $H$ отрицательно определена — локальный **максимум**.
        *   Если знакопеременна — **седловая точка** (не экстремум).

*   **Формула:**
    Матрица Гессе:
    $$H = \begin{pmatrix} \frac{\partial^2 f}{\partial x_1^2} & \dots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial^2 f}{\partial x_n \partial x_1} & \dots & \frac{\partial^2 f}{\partial x_n^2} \end{pmatrix}$$

*   **Примеры:**
    Анализ ландшафта функции потерь при обучении нейронных сетей для определения того, находится ли алгоритм в минимуме или застрял на седловой точке.

---

### Билет №15

**1. Метод золотого сечения. Сравнение методов нулевого порядка**

*   **Теоретическая часть:**
    (См. ответы в Билетах №4, №6, №9).
    Метод золотого сечения — надежный метод одномерной минимизации унимодальных функций. Разбивает отрезок в отношении $\Phi \approx 1.618$.
    *Сравнение:*
    *   *Равномерный поиск:* Самый надежный, но самый медленный.
    *   *Дихотомия:* Быстрая, но требует вычисления двух точек на каждом шаге.
    *   *Золотое сечение:* Оптимально по соотношению "скорость/затраты" (1 вычисление на шаг, сокращение интервала в 1.618 раз).
    *   *Фибоначчи:* Самое эффективное, но сложнее в реализации (нужно знать $N$ заранее).

*   **Формула:**
    Длина интервала на шаге $N$: $L_N = L_0 \cdot (0.618)^{N-1}$.

*   **Примеры:**
    Оптимизация параметров цифровых фильтров в электронике, где функция качества задана не аналитически, а как результат симуляции работы схемы.

**2. Нелинейное программирование. Проекционные методы**

*   **Теоретическая часть:**
    Проекционные методы (например, метод проекции градиента Розена) предназначены для решения задач условной оптимизации, где ограничения заданы линейно.
    Идея: движение осуществляется в направлении антиградиента. Если траектория упирается в границу допустимой области (ограничение), то вектор антиградиента *проецируется* на эту границу (гиперплоскость ограничений). Дальнейшее движение происходит вдоль границы ("скольжение" по ограничению) до тех пор, пока не будет найдена оптимальная точка.

*   **Формула:**
    Оператор проектирования $P$ на подпространство ограничений:
    $$\vec{p}_k = -P \cdot \nabla f(\vec{x}_k)$$
    где $P = I - A^T(A A^T)^{-1}A$ (матрица проектирования), $A$ — матрица активных ограничений.
    Новая точка: $\vec{x}_{k+1} = \vec{x}_k + \lambda \vec{p}_k$.

*   **Примеры:**
    Оптимизация траекторий роботов-манипуляторов, где движения ограничены физическими барьерами (стенами, столом). Алгоритм "скользит" вдоль препятствия.

---

### Билет №16

**1. Основные элементы постановки задачи оптимизации**

*   **Теоретическая часть:**
    (См. ответ в Билете №12).
    Задача оптимизации формально определяется тремя компонентами:
    1.  **Вектор варьируемых параметров $\vec{u}$**: Независимые переменные, которые мы меняем.
    2.  **Целевая функция (функционал) $J(\vec{u})$**: Критерий эффективности, который нужно экстремизировать (например, $J \to \min$).
    3.  **Ограничения $G_j(\vec{u})$**: Условия, определяющие допустимую область $U$. Могут быть типа равенств или неравенств.
    Оптимизация — это выбор наилучшего варианта из множества возможных.

*   **Примеры:**
    Экономика: $\vec{u}$ — объемы закупок товаров, $J$ — прибыль (макс), Ограничения — бюджет, складские места.

**2. Нелинейное программирование. Методы штрафных функций**

*   **Теоретическая часть:**
    Методы штрафных функций сводят задачу *условной* оптимизации к задаче *безусловной* оптимизации.
    Идея: к исходной целевой функции $f(x)$ добавляется слагаемое (штраф), которое велико, если нарушаются ограничения, и равно нулю (или мало), если ограничения выполняются.
    Виды:
    *   *Внешние штрафы (Метод штрафов):* Штраф начисляется только за пределами допустимой области. Решение приближается к оптимуму извне.
    *   *Внутренние штрафы (Метод барьеров):* Штраф возрастает при приближении к границе изнутри. Точка всегда остается в допустимой области.

*   **Формула:**
    Вспомогательная функция (для внешнего штрафа):
    $$\Phi(\vec{x}, r) = f(\vec{x}) + r \sum_{j} \max(0, G_j(\vec{x}))^2$$
    где $r$ — коэффициент штрафа. Алгоритм: решается задача $\min \Phi$ при последовательном увеличении $r \to \infty$.

*   **Примеры:**
    Оптимизация формы детали в CAD-системах, где деталь не должна выходить за пределы заданного габаритного контейнера. Выход за границы сильно "штрафуется" в целевой функции.

---

### Билет №17

**1. Общая характеристика одномерных методов оптимизации**

*   **Теоретическая часть:**
    Одномерная оптимизация (поиск минимума функции одной переменной $f(x)$) является базовым блоком для многомерных методов (линейный поиск).
    Методы делятся на классы по используемой информации:
    *   **Нулевого порядка (прямые):** Используют только значения функции. Требуют унимодальности. (Дихотомия, Золотое сечение, Фибоначчи). Надежные, но медленные.
    *   **Первого порядка:** Используют производные $f'(x)$. (Метод средней точки, метод секущих). Более быстрые.
    *   **Второго порядка:** Используют $f''(x)$. (Метод Ньютона). Самые быстрые (квадратичная сходимость), но требуют гладкости функции и вычисления второй производной.
    *   **Полиномиальная аппроксимация:** (Метод Пауэлла). Аппроксимация функции параболой.

*   **Примеры:**
    Линейный поиск шага $\lambda$ в методе наискорейшего спуска для многомерной функции.

**2. Метод деформируемого многогранника (Метод Нелдера Мида)**

*   **Теоретическая часть:**
    (См. ответ в Билете №10 и №18).
    Это эвристический метод безусловной оптимизации функции $n$ переменных. Не требует вычисления градиентов.
    Оперирует симплексом — фигурой из $n+1$ точек. На каждом шаге меняется худшая точка.
    Основные операции трансформации симплекса:
    1.  *Отражение:* перенос худшей точки через центр тяжести остальных.
    2.  *Растяжение:* если отражение дало очень хороший результат, двигаемся дальше в том же направлении.
    3.  *Сжатие:* если отражение не дало результата, пробуем точку ближе к центру.
    4.  *Редукция:* сжатие всего симплекса к лучшей вершине (используется редко, в случае "застревания").
    Метод эффективен для функций с "оврагами" и шумом, но не гарантирует сходимость к глобальному минимуму.

*   **Формула:**
    Центр тяжести грани: $\vec{x}_c = \frac{1}{n}\sum \vec{x}_{best}$.
    Отраженная точка: $\vec{x}_r = \vec{x}_c + \alpha(\vec{x}_c - \vec{x}_{worst})$.

---

### Билет №18

**1. Метод деформируемого многогранника (Метод Нелдера Мида)**

*   **Теоретическая часть:**
    (См. ответ в Билетах №10, 17).
    Метод нулевого порядка, основанный на адаптации геометрии симплекса к топологии целевой функции. Симплекс "перекатывается", вытягивается вдоль оврагов и сжимается в окрестности минимума. Это делает его одним из самых популярных методов для задач малой размерности ($n < 10..20$) с негладкими функциями.

**2. Нелинейное программирование. Метод последовательной линеаризации**

*   **Теоретическая часть:**
    Метод последовательной линеаризации (или метод Эрроу-Гурвица, или приближение Ньютона для задач с ограничениями) предназначен для решения задач нелинейного программирования.
    Идея: сложная нелинейная задача заменяется последовательностью более простых *линейных* задач (задач линейного программирования - ЛП).
    На каждом шаге нелинейная целевая функция и нелинейные ограничения линеаризуются (раскладываются в ряд Тейлора до линейных членов) в окрестности текущей точки. Полученная задача ЛП решается Симплекс-методом. Найденное решение определяет направление движения.

*   **Формула:**
    Исходная задача: $\min f(x)$ при $g(x) \le 0$.
    Линеаризация в точке $x_k$:
    $f(x) \approx f(x_k) + \nabla f(x_k)^T (x - x_k)$
    $g(x) \approx g(x_k) + \nabla g(x_k)^T (x - x_k) \le 0$
    Решаем задачу ЛП относительно приращения $\Delta x = x - x_k$.

*   **Примеры:**
    Оптимизация технологических процессов в химической промышленности, где зависимости сложны и нелинейны, но на малых шагах их можно считать линейными.

---

### Билет №19

**1. Метод золотого сечения. Сравнение методов нулевого порядка**

*   **Теоретическая часть:**
    (См. ответ в Билете №4 и №9).
    Метод золотого сечения является эталоном эффективности для методов, не использующих производные и не знающих заранее число итераций.
    Сравнение:
    *   *Дихотомия:* Надежная, но менее эффективная (2 вычисления на шаг).
    *   *Золотое сечение:* Эффективнее дихотомии (1 вычисление на шаг, коэффициент сжатия 0.618).
    *   *Фибоначчи:* Самый теоретически эффективный, но неудобен на практике из-за фиксированного $N$.

**2. Анализ чувствительности и его применение в проектировании и инженерном деле**

*   **Теоретическая часть:**
    Анализ чувствительности исследует, как изменение входных параметров $\vec{u}$ влияет на целевую функцию $J(\vec{u})$ и ограничения.
    Цель:
    1.  Определить наиболее значимые параметры (которые сильно влияют на качество).
    2.  Оценить устойчивость оптимального решения к погрешностям изготовления или внешним условиям.
    Подходы:
    *   *Прямой метод (конечные разности):* Меняем каждый параметр на $\Delta u_i$ и смотрим на результат. Просто, но дорого вычислительно.
    *   *Метод сопряженной переменной:* Позволяет вычислить градиент чувствительности по всем параметрам за один расчет сопряженной задачи.

*   **Примеры:**
    В автомобилестроении: как изменение толщины металла кузова в разных местах (параметры) повлияет на безопасность при ударе (целевая функция). Анализ чувствительности покажет, какие детали критически важны (их нельзя делать тоньше), а на каких можно сэкономить материал.

---

### Билет №20

**1. Метод дихотомии. Сравнение с равномерным делением**

*   **Теоретическая часть:**
    (См. ответ в Билете №5).
    Метод дихотомии (половинного деления) последовательно сужает интервал неопределенности в 2 раза, используя две пробные точки вокруг центра интервала.
    Сравнение:
    *   *Равномерное деление:* Разбивает отрезок на $N$ частей сразу. Надежен для неунимодальных функций, но крайне неэффективен для высокой точности.
    *   *Дихотомия:* Требует унимодальности. Значительно быстрее достигает высокой точности (логарифмическая зависимость числа шагов от точности против линейной у равномерного поиска).

**2. Три подхода к анализу чувствительности. Сравнение**

*   **Теоретическая часть:**
    Для оценки производных целевой функции по параметрам $\frac{dJ}{du_i}$ используются три метода:
    1.  **Метод конечных разностей:** Параметр $u_i$ возмущается на малую величину $\delta u$, решается задача анализа. $\frac{dJ}{du} \approx \frac{J(u+\delta u) - J(u)}{\delta u}$.
        *   *Плюс:* Прост в реализации.
        *   *Минус:* Очень медленный (требует $N+1$ решений задачи для $N$ параметров), проблемы с точностью.
    2.  **Прямое дифференцирование уравнений состояния:** Дифференцируются уравнения модели по параметру.
        *   *Плюс:* Точнее разностей.
        *   *Минус:* Требует $N$ решений системы уравнений чувствительности.
    3.  **Метод сопряженной переменной (Adjoint method):**
        *   *Плюс:* Самый эффективный. Градиент по *всем* параметрам вычисляется за *одно* решение основной и *одно* решение сопряженной задачи, независимо от числа параметров $N$.
        *   *Минус:* Сложен математически и в реализации.

*   **Примеры:**
    Аэродинамическая оптимизация формы крыла (тысячи параметров формы). Использовать конечные разности невозможно (слишком долго), применяют только метод сопряженной переменной.

---

### Билет №21

**1. Общая характеристика одномерных методов оптимизации нулевого порядка**

*   **Теоретическая часть:**
    (См. ответ в Билете №6 и №10).
    Методы поиска экстремума функции одной переменной без производных.
    Основа: свойство унимодальности.
    Классы:
    *   *Одновременный поиск:* Метод равномерного перебора.
    *   *Последовательный поиск (симметричные методы):* Дихотомия, Золотое сечение, Фибоначчи.
    Все эти методы гарантированно сходятся к локальному экстремуму унимодальной функции с заданной точностью.

**2. Метод сопряженных переменных в анализе чувствительности**

*   **Теоретическая часть:**
    Метод сопряженных переменных (Adjoint method) — наиболее мощный инструмент для расчета градиента целевой функции (чувствительности) в задачах с большим количеством параметров проектирования.
    Суть: вместо того чтобы вычислять влияние каждого параметра на результат по отдельности, вводится вспомогательный вектор — *сопряженная переменная* $\vec{\lambda}$. Она определяется из решения линейной системы уравнений, сопряженной к линеаризованной системе уравнений состояния объекта.
    После нахождения $\vec{\lambda}$, полная производная функционала по параметрам выражается через простую формулу, включающую $\vec{\lambda}$ и частные производные функций системы.

*   **Формула:**
    Если задача: $\min J(\vec{y}, \vec{u})$ при $F(\vec{y}, \vec{u}) = 0$.
    Сопряженное уравнение:
    $$[\frac{\partial F}{\partial \vec{y}}]^T \vec{\lambda} = -\frac{\partial J}{\partial \vec{y}}$$
    Градиент (чувствительность):
    $$\frac{dJ}{d\vec{u}} = \frac{\partial J}{\partial \vec{u}} + \vec{\lambda}^T \frac{\partial F}{\partial \vec{u}}$$

*   **Примеры:**
    Оптимизация формы в гидрогазодинамике (CFD), топологическая оптимизация в механике, где число переменных может достигать миллионов (плотности в каждом конечном элементе).

---

### Билет №22

**1. Общая характеристика одномерных методов**

*   **Теоретическая часть:**
    (Обобщающий вопрос).
    Методы одномерной оптимизации классифицируются по использованию производных:
    1.  *Нулевого порядка:* (Дихотомия, Золотое сечение). Надежные, работают для негладких функций.
    2.  *Первого порядка:* (Метод средней точки, метод секущих). Требуют $f'$, сходятся быстрее.
    3.  *Второго порядка:* (Метод Ньютона). Требуют $f''$, квадратичная сходимость.
    4.  *Полиномиальная аппроксимация:* (Пауэлла). Строит параболу по точкам.
    Основное применение этих методов — вспомогательный этап "линейного поиска" (line search) в многомерных алгоритмах для определения длины шага.

**2. Метод сопряженных градиентов (Метод Флетчера Ривза)**

*   **Теоретическая часть:**
    (См. ответ в Билете №8).
    Метод Флетчера-Ривса — алгоритм безусловной многомерной оптимизации первого порядка. Он развивает метод наискорейшего спуска.
    Проблема наискорейшего спуска: в оврагах векторы градиентов на соседних шагах ортогональны, что приводит к "зигзагам" и медленной сходимости.
    Решение: Метод Флетчера-Ривса строит направления спуска $\vec{p}_k$, которые являются *сопряженными* (т.е. ортогональными относительно матрицы Гессе функции). Это "выпрямляет" траекторию движения по дну оврага. Для квадратичной функции $n$ переменных метод находит точный минимум не более чем за $n$ шагов.

*   **Формула:**
    (См. формулы в билете №8).
    Пересчет направления: $\vec{p}_{k+1} = -\vec{g}_{k+1} + \beta_k \vec{p}_k$, где $\vec{g} = \nabla f$.

*   **Примеры:**
    Решение больших разреженных систем уравнений, возникающих при конечно-элементном моделировании упругости.

---

### Билет №23

**1. Необходимые и достаточные условия в одномерных задачах оптимизации**

*   **Теоретическая часть:**
    (См. ответ в Билете №11).
    Для функции $y=f(x)$:
    *   *Необходимое условие экстремума:* $f'(x) = 0$.
    *   *Достаточные условия:*
        *   Если $f'(x_0)=0$ и $f''(x_0) > 0$ $\Rightarrow$ локальный минимум.
        *   Если $f'(x_0)=0$ и $f''(x_0) < 0$ $\Rightarrow$ локальный максимум.
        *   Если $f'(x_0)=0$ и $f''(x_0) = 0$ $\Rightarrow$ требуется исследование высших производных или знака приращения функции.

**2. Нелинейное программирование. Метод последовательной линеаризации**

*   **Теоретическая часть:**
    (См. ответ в Билете №18).
    Метод решения задач условной нелинейной оптимизации. Задача заменяется серией линейных приближений.
    На каждой итерации $k$:
    1.  Целевая функция и ограничения разлагаются в ряд Тейлора в точке $\vec{x}_k$, оставляя только линейные члены.
    2.  Получается задача Линейного Программирования (ЗЛП) относительно приращения $\Delta \vec{x}$.
    3.  ЗЛП решается симплекс-методом.
    4.  Делается шаг в найденном направлении.
    Метод эффективен, когда нелинейности "плавные", и близок к методу Ньютона по скорости сходимости.

*   **Примеры:**
    Оптимизация ферменных конструкций, где ограничения по напряжениям являются нелинейными, но близкими к линейным при малых изменениях сечений.

---

### Билет №24

**1. Основные элементы постановки задачи оптимизации**

*   **Теоретическая часть:**
    (См. ответ в Билетах №12 и №16).
    1.  **Параметры управления ($\vec{u}$):** То, что меняем.
    2.  **Целевая функция ($J$):** То, что хотим улучшить (минимизировать).
    3.  **Ограничения:**
        *   Равенства $h_i(\vec{u}) = 0$ (например, законы физики, уравнения равновесия).
        *   Неравенства $g_j(\vec{u}) \le 0$ (ресурсные ограничения, прочность).
        *   Граничные $u_{min} \le u \le u_{max}$.
    4.  **Математическая модель:** Связь между $\vec{u}$ и физическим состоянием системы.

**2. Метод сопряженных градиентов (Метод Флетчера Ривза)**

*   **Теоретическая часть:**
    (См. ответ в Билетах №8 и №22).
    Метод первого порядка, использующий понятие сопряженных направлений.
    Ключевые особенности:
    *   Не требует хранения матрицы Гессе (в отличие от Ньютона), что экономит память.
    *   Сходится быстрее наискорейшего спуска (суперлинейная сходимость).
    *   На каждом шаге требуется точный линейный поиск (нахождение минимума функции вдоль выбранного направления).
    *   Раз в $n$ шагов рекомендуется делать "рестарт" (сброс направления на антиградиент) для удаления накопленных ошибок.

*   **Формула:**
    $\beta_k = \|\nabla f_{k+1}\|^2 / \|\nabla f_k\|^2$.

*   **Примеры:**
    Применяется в задачах, где число переменных $n$ велико ($10^3 - 10^6$), и хранение матрицы $n \times n$ невозможно. Например, метеорология или обработка больших данных.
